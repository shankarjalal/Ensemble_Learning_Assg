{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49abaa5",
   "metadata": {},
   "source": [
    "# **Ensamble Techinques Assg**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed31bee",
   "metadata": {},
   "source": [
    "1. Can we use Bagging for regression problems?\n",
    "\n",
    "Ans- Yes, Bagging can be used for regression problems. In Bagging Regressor, predictions from multiple\n",
    "base regressors are averaged to produce the final output.\n",
    "\n",
    "2. Difference between single model training and multiple model training\n",
    "\n",
    "Ans- Single model training uses one algorithm and may suffer from high bias or variance. Multiple model\n",
    "training (ensemble) combines several models to improve accuracy and stability.\n",
    "\n",
    "3. Feature randomness in Random Forest\n",
    "\n",
    "Random Forest selects a random subset of features at each split, which reduces correlation among\n",
    "trees and improves generalization.\n",
    "\n",
    "4. Out-of-Bag (OOB) Score\n",
    "\n",
    "OOB score evaluates model performance using samples not selected during bootstrap sampling. It\n",
    "works as an internal validation method.\n",
    "\n",
    "5. Feature importance in Random Forest\n",
    "\n",
    "Feature importance is calculated based on how much a feature reduces impurity across all trees.\n",
    "\n",
    "6. Working principle of Bagging Classifier\n",
    "\n",
    "Multiple models are trained on bootstrap samples and final prediction is obtained using majority\n",
    "voting.\n",
    "\n",
    "7. Evaluation of Bagging Classifier\n",
    "\n",
    "\n",
    "Bagging Classifier is evaluated using Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
    "\n",
    "8. Working of Bagging Regressor\n",
    "\n",
    "Bagging Regressor trains multiple regressors and averages their predictions to reduce variance.\n",
    "\n",
    "9. Main advantage of ensemble techniques\n",
    "\n",
    "Ensemble techniques improve prediction accuracy and reduce overfitting.\n",
    "\n",
    "10. Main challenge of ensemble methods\n",
    "\n",
    "They increase computational cost and reduce model interpretability.\n",
    "\n",
    "11. Key idea behind ensemble techniques\n",
    "\n",
    "Combining multiple weak learners to create a strong learner.\n",
    "\n",
    "12. Random Forest Classifier\n",
    "\n",
    "It is an ensemble of decision trees using bagging and feature randomness for classification tasks.\n",
    "\n",
    "13. Types of ensemble techniques\n",
    "\n",
    "Bagging, Boosting, and Stacking.\n",
    "\n",
    "14. Ensemble learning definition\n",
    "\n",
    "Ensemble learning is a technique where multiple models are combined to improve performance.\n",
    "\n",
    "15. When should we avoid ensemble methods\n",
    "\n",
    "When data is small, real-time prediction is required, or interpretability is critical.\n",
    "\n",
    "16. How Bagging reduces overfitting\n",
    "\n",
    "By averaging predictions from multiple models, bagging reduces variance.\n",
    "\n",
    "17. Why Random Forest is better than Decision Tree\n",
    "\n",
    "Random Forest reduces overfitting and provides better generalization than a single decision tree.\n",
    "\n",
    "18. Role of bootstrap sampling in Bagging\n",
    "\n",
    "Bootstrap sampling creates diverse training datasets for base models.\n",
    "\n",
    "19. Real-world applications of ensemble techniques\n",
    "\n",
    "Fraud detection, medical diagnosis, recommendation systems, and stock prediction.\n",
    "\n",
    "20. Difference between Bagging and Boosting\n",
    "\n",
    "Bagging reduces variance using parallel models, while Boosting reduces bias using sequential\n",
    "learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4818ff",
   "metadata": {},
   "source": [
    "## **Prectical Questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ce1c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy ?\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Bagging Classifier with Decision Trees as base estimators\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "bagging_clf = BaggingClassifier(estimator=base_estimator, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the Bagging Classifier\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be199ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1305.76\n"
     ]
    }
   ],
   "source": [
    "# 22) Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE) ?\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load a sample regression dataset.\n",
    "X, y = make_regression(n_samples=100, n_features=4, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Bagging Regressor with Decision Trees as base estimators.\n",
    "base_est = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "\n",
    "# Create the Bagging Regressor\n",
    "bagging_reg = BaggingRegressor(estimator=base_est, n_estimators=50, random_state=42)\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set.\n",
    "y_pred_rg = bagging_reg.predict(X_test)\n",
    "# Calculate and print the Mean Squared Error (MSE).\n",
    "mse = mean_squared_error(y_test, y_pred_rg)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Accuracy: 97.08%\n"
     ]
    }
   ],
   "source": [
    "# 23) Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores?\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Model Accuracy: {accuracy_rf * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Mean Squared Error: 1177.36\n",
      "Decision Tree Mean Squared Error: 2539.15\n",
      "Random Forest Regressor performs better than Decision Tree Regressor.\n"
     ]
    }
   ],
   "source": [
    "#24) Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load a sample regression dataset.\n",
    "X, y = make_regression(n_samples=100, n_features=4, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "# Make predictions on the test set using Random Forest\n",
    "y_pred_rf_reg = rf_reg.predict(X_test)\n",
    "# Calculate Mean Squared Error for Random Forest\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf_reg)\n",
    "print(f\"Random Forest Mean Squared Error: {mse_rf:.2f}\")\n",
    "\n",
    "# Create a single Decision Tree Regressor\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "# Make predictions on the test set using Decision Tree\n",
    "y_pred_dt = dt_reg.predict(X_test)\n",
    "# Calculate Mean Squared Error for Decision Tree\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree Mean Squared Error: {mse_dt:.2f}\")\n",
    "\n",
    "# Compare performances\n",
    "if mse_rf < mse_dt:\n",
    "    print(\"Random Forest Regressor performs better than Decision Tree Regressor.\")\n",
    "else:\n",
    "    print(\"Decision Tree Regressor performs better than Random Forest Regressor.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf797524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-Bag Score: 94.29%\n"
     ]
    }
   ],
   "source": [
    "# 25 ( 2 Compute the Out-of-Bag (OOB) Score for a Random Forest.\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier with OOB score enabled\n",
    "rf_clf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_clf_oob.fit(X_train, y_train)\n",
    "\n",
    "# Print the OOB score\n",
    "print(f\"Out-of-Bag Score: {rf_clf_oob.oob_score_ * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be76670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with SVM Model Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 26) Train a Bagging Classifier using SVM as a base estimator and print accuracy2 ?\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset.\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "# Split the dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Create a Bagging Classifier with SVM as base estimators.\n",
    "base_estimator_svm = SVC(probability=True)\n",
    "bagging_clf_svm = BaggingClassifier(estimator=base_estimator_svm, n_estimators=50, random_state=42)\n",
    "# Train the Bagging Classifier.\n",
    "bagging_clf_svm.fit(X_train, y_train)\n",
    "# Make predictions on the test set.\n",
    "y_pred_svm = bagging_clf_svm.predict(X_test)\n",
    "# Calculate and print the accuracy.\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Bagging Classifier with SVM Model Accuracy: {accuracy_svm * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce92b4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with 10 trees Accuracy: 100.00%\n",
      "Random Forest with 50 trees Accuracy: 100.00%\n",
      "Random Forest with 100 trees Accuracy: 100.00%\n",
      "Random Forest with 200 trees Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 26) Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "tree_counts = [10, 50, 100, 200]\n",
    "for n_trees in tree_counts:\n",
    "    rf_clf_trees = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
    "    rf_clf_trees.fit(X_train, y_train)\n",
    "    y_pred_trees = rf_clf_trees.predict(X_test)\n",
    "    accuracy_trees = accuracy_score(y_test, y_pred_trees)\n",
    "    print(f\"Random Forest with {n_trees} trees Accuracy: {accuracy_trees * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2f37ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier with Logistic Regression AUC Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# 27)Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Load the dataset.\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "# Split the dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Create a Bagging Classifier with Logistic Regression as base estimators.\n",
    "base_estimator_lr = LogisticRegression(max_iter=200)\n",
    "bagging_clf_lr = BaggingClassifier(estimator=base_estimator_lr, n_estimators=50, random_state=42)\n",
    "# Train the Bagging Classifier.\n",
    "bagging_clf_lr.fit(X_train, y_train)\n",
    "# Make probability predictions on the test set.\n",
    "y_prob_lr = bagging_clf_lr.predict_proba(X_test)\n",
    "# Calculate and print the AUC score.\n",
    "auc_score_lr = roc_auc_score(y_test, y_prob_lr, multi_class='ovr')\n",
    "print(f\"Bagging Classifier with Logistic Regression AUC Score: {auc_score_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8f7a739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Mean Squared Error: 1177.36\n"
     ]
    }
   ],
   "source": [
    "# 28) Train a Random Forest Regressor and analyze feature importance scores\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Load a sample regression dataset.\n",
    "X, y = make_regression(n_samples=100, n_features=4, noise=0.1, random_state=42)\n",
    "# Split the dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set.\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "# Calculate and print the Mean Squared Error (MSE).\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Regressor Mean Squared Error: {mse_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b9fa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 100.00%\n",
      "Random Forest Classifier Accuracy: 100.00%\n",
      "Both classifiers have the same accuracy.\n"
     ]
    }
   ],
   "source": [
    "# 30 Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Create a Bagging Classifier\n",
    "bagging_clf_final = BaggingClassifier(n_estimators=50, random_state=42)\n",
    "bagging_clf_final.fit(X_train, y_train)\n",
    "# Make predictions on the test set using Bagging\n",
    "y_pred_bagging = bagging_clf_final.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy_bagging * 100:.2f}%\")\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_clf_final = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_final.fit(X_train, y_train)\n",
    "# Make predictions on the test set using Random Forest\n",
    "y_pred_rf_final = rf_clf_final.predict(X_test)\n",
    "accuracy_rf_final = accuracy_score(y_test, y_pred_rf_final)\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy_rf_final * 100:.2f}%\")\n",
    "\n",
    "# Compare performances\n",
    "if accuracy_bagging > accuracy_rf_final:\n",
    "    print(\"Bagging Classifier performs better than Random Forest Classifier.\")\n",
    "elif accuracy_rf_final > accuracy_bagging:\n",
    "    print(\"Random Forest Classifier performs better than Bagging Classifier.\")\n",
    "else:\n",
    "    print(\"Both classifiers have the same accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32) Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_clf_grid = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid.\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10,  20],\n",
    "    'min_samples_split' : [2,5,1],\n",
    "    'min_samples_leaf' : [1,2,4] # Corrected typo here\n",
    "    }\n",
    "\n",
    "# Create a GridSearchCV object.\n",
    "grid_search = GridSearchCV(estimator=rf_clf_grid, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the training data.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator from the grid search.\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best estimator.\n",
    "y_pred_grid = best_rf_clf.predict(X_test)\n",
    "\n",
    "# Check accuracy of this model\n",
    "acc = accuracy_score(y_pred_grid, y_test)\n",
    "\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Accuracy on Test Set: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03726a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33) Train a Bagging Regressor with different numbers of base estimators and compare performance.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.2, random_state=42)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Prepare Data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and Evaluate models\n",
    "n_estimators_range = range(1, 11)\n",
    "mse_scores = []\n",
    "\n",
    "for n_estimators_val in n_estimators_range:\n",
    "    base_est = DecisionTreeRegressor(random_state=42)\n",
    "    bag_re = BaggingRegressor(estimator=base_est, n_estimator=n_estimators_val, ramdom_state=42)\n",
    "    bag_re.fit(X_train, y_train)\n",
    "    y_pred = bag_re.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "print('MSe scores for different n_estimators:')\n",
    "for i, mse in enumerate(mse_scores):\n",
    "    print(f\"n_estimators={n_estimators_range[i]} : MSE = {mse:.2f}\")\n",
    "\n",
    "\n",
    "# Visualize Performance\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(n_estimators_range, mse_scores, marker='o', linestyle='-',, color='skyblue', label = 'MSE Score')\n",
    "plt.xlabel('Number of Est.')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Bagging Reg Performance vs. Number of Estimators')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
